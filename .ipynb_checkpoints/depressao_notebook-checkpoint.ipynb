{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import configparser as cp\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#ALGORITMOS SELECIONADOS ATÉ O MOMENTO\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#-----------------------------------------\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = cp.ConfigParser()\n",
    "config.read('config.ini')\n",
    "api_key = config['twitter_credenciais']['api_key']\n",
    "api_key_secret = config['twitter_credenciais']['api_key_secret']\n",
    "access_token = config['twitter_credenciais']['access_token']\n",
    "access_token_secret = config['twitter_credenciais']['access_token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_depressao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tweet</th>\n",
       "      <th>ds_tweet</th>\n",
       "      <th>fisiologico</th>\n",
       "      <th>psiquico</th>\n",
       "      <th>comportamental</th>\n",
       "      <th>label</th>\n",
       "      <th>validado_espec</th>\n",
       "      <th>dh_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3696</td>\n",
       "      <td>dormi o dia inteiro tinha horas w meu desperta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-09 02:32:41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885</td>\n",
       "      <td>já entendi que eu não sou importante tá bom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 12:15:04.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3888</td>\n",
       "      <td>chorando pq não sou importante pra ngm 👍🏿 http...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 23:56:04.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3889</td>\n",
       "      <td>Sinto sempre que nao sou importante para certa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 14:43:38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8369</td>\n",
       "      <td>Tô indo pra escola com a força do ódio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 12:10:22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>10629</td>\n",
       "      <td>@LUCAztec como ousa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01 00:58:30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>10630</td>\n",
       "      <td>@tatastew aí amiga mas pelo menos ele lembra K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 17:37:21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>10631</td>\n",
       "      <td>eu só quero chegar em casa, tomar um banho e m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 16:07:31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>10632</td>\n",
       "      <td>vou ter que dar um menos no café porque se pá ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-11 11:42:42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>10633</td>\n",
       "      <td>@lxcxzs irmão lázaro???</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 15:43:28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4050 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_tweet                                           ds_tweet  \\\n",
       "0         3696  dormi o dia inteiro tinha horas w meu desperta...   \n",
       "1         3885        já entendi que eu não sou importante tá bom   \n",
       "2         3888  chorando pq não sou importante pra ngm 👍🏿 http...   \n",
       "3         3889  Sinto sempre que nao sou importante para certa...   \n",
       "4         8369             Tô indo pra escola com a força do ódio   \n",
       "...        ...                                                ...   \n",
       "4045     10629                                @LUCAztec como ousa   \n",
       "4046     10630  @tatastew aí amiga mas pelo menos ele lembra K...   \n",
       "4047     10631  eu só quero chegar em casa, tomar um banho e m...   \n",
       "4048     10632  vou ter que dar um menos no café porque se pá ...   \n",
       "4049     10633                            @lxcxzs irmão lázaro???   \n",
       "\n",
       "      fisiologico  psiquico  comportamental  label  validado_espec  \\\n",
       "0               1         1               0      1               1   \n",
       "1               0         1               0      3               1   \n",
       "2               0         1               0      3               1   \n",
       "3               0         1               0      3               1   \n",
       "4               0         0               0      0               1   \n",
       "...           ...       ...             ...    ...             ...   \n",
       "4045            0         0               0      0               0   \n",
       "4046            0         0               0      0               0   \n",
       "4047            0         0               0      0               0   \n",
       "4048            0         0               0      0               0   \n",
       "4049            0         0               0      0               0   \n",
       "\n",
       "                        dh_tweet  \n",
       "0     2019-10-09 02:32:41.000000  \n",
       "1     2019-10-12 12:15:04.000000  \n",
       "2     2019-10-11 23:56:04.000000  \n",
       "3     2019-10-12 14:43:38.000000  \n",
       "4     2019-10-11 12:10:22.000000  \n",
       "...                          ...  \n",
       "4045  2019-11-01 00:58:30.000000  \n",
       "4046  2019-10-31 17:37:21.000000  \n",
       "4047  2019-10-31 16:07:31.000000  \n",
       "4048  2019-11-11 11:42:42.000000  \n",
       "4049  2019-11-05 15:43:28.000000  \n",
       "\n",
       "[4050 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_postagem'] = pd.to_datetime(df['dh_tweet']).dt.date\n",
    "df['hora_postagem'] = pd.to_datetime(df['dh_tweet']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_tweets(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9]_+', '', text)\n",
    "    text = re.sub(r'@_[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'@_[A-Za-z0-9]_+', '', text)# Remove mencoes\n",
    "    text = re.sub(r'#','', text) # Remove simbolo de hashtags\n",
    "    text = re.sub(r'RT[\\s]+','',text) # Remove Retweets\n",
    "    text = re.sub(r'https?:/\\/\\S+','',text) # Remove URLS\n",
    "\n",
    "    regrex_pattern = re.compile(pattern = \"[\"   #Remove todos os emojis\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4050"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets'] = df['ds_tweet'].apply(limpar_tweets)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4049"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df['tweets'] == ''].index)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"portuguese\")\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removendo os stopwords\n",
    "df[\"tweets\"] = df[\"tweets\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       dormi dia inteiro horas w despertafor tocava a...\n",
       "1                               entendi importante tá bom\n",
       "2                          chorando pq importante pra ngm\n",
       "3       Sinto sempre nao importante certas pessoas man...\n",
       "4                           Tô indo pra escola força ódio\n",
       "                              ...                        \n",
       "4045                                                 ousa\n",
       "4046    aí amiga menos lembra KKKKKKK lucas noia carai...\n",
       "4047           quero chegar casa, tomar banho enfiar cama\n",
       "4048    vou ter dar menos café porque pá quantidade an...\n",
       "4049                                      irmão lázaro???\n",
       "Name: tweets, Length: 4049, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[\"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    return \" \".join(lemma_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tweet</th>\n",
       "      <th>ds_tweet</th>\n",
       "      <th>fisiologico</th>\n",
       "      <th>psiquico</th>\n",
       "      <th>comportamental</th>\n",
       "      <th>label</th>\n",
       "      <th>validado_espec</th>\n",
       "      <th>dh_tweet</th>\n",
       "      <th>data_postagem</th>\n",
       "      <th>hora_postagem</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3696</td>\n",
       "      <td>dormi o dia inteiro tinha horas w meu desperta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-09 02:32:41.000000</td>\n",
       "      <td>2019-10-09</td>\n",
       "      <td>02:32:41</td>\n",
       "      <td>dormi dia inteiro horas w despertafor tocava a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885</td>\n",
       "      <td>já entendi que eu não sou importante tá bom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 12:15:04.000000</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>12:15:04</td>\n",
       "      <td>entendi importante tá bom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3888</td>\n",
       "      <td>chorando pq não sou importante pra ngm 👍🏿 http...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 23:56:04.000000</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>23:56:04</td>\n",
       "      <td>chorando pq importante pra ngm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3889</td>\n",
       "      <td>Sinto sempre que nao sou importante para certa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 14:43:38.000000</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>14:43:38</td>\n",
       "      <td>Sinto sempre nao importante certas pessoas man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8369</td>\n",
       "      <td>Tô indo pra escola com a força do ódio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 12:10:22.000000</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>12:10:22</td>\n",
       "      <td>Tô indo pra escola força ódio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>10629</td>\n",
       "      <td>@LUCAztec como ousa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01 00:58:30.000000</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>00:58:30</td>\n",
       "      <td>ousa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>10630</td>\n",
       "      <td>@tatastew aí amiga mas pelo menos ele lembra K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 17:37:21.000000</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>17:37:21</td>\n",
       "      <td>aí amiga menos lembra KKKKKKK lucas noia carai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>10631</td>\n",
       "      <td>eu só quero chegar em casa, tomar um banho e m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 16:07:31.000000</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>16:07:31</td>\n",
       "      <td>quero chegar casa, tomar banho enfiar cama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>10632</td>\n",
       "      <td>vou ter que dar um menos no café porque se pá ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-11 11:42:42.000000</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>11:42:42</td>\n",
       "      <td>vou ter dar menos café porque pá quantidade an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>10633</td>\n",
       "      <td>@lxcxzs irmão lázaro???</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 15:43:28.000000</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>15:43:28</td>\n",
       "      <td>irmão lázaro???</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4049 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_tweet                                           ds_tweet  \\\n",
       "0         3696  dormi o dia inteiro tinha horas w meu desperta...   \n",
       "1         3885        já entendi que eu não sou importante tá bom   \n",
       "2         3888  chorando pq não sou importante pra ngm 👍🏿 http...   \n",
       "3         3889  Sinto sempre que nao sou importante para certa...   \n",
       "4         8369             Tô indo pra escola com a força do ódio   \n",
       "...        ...                                                ...   \n",
       "4045     10629                                @LUCAztec como ousa   \n",
       "4046     10630  @tatastew aí amiga mas pelo menos ele lembra K...   \n",
       "4047     10631  eu só quero chegar em casa, tomar um banho e m...   \n",
       "4048     10632  vou ter que dar um menos no café porque se pá ...   \n",
       "4049     10633                            @lxcxzs irmão lázaro???   \n",
       "\n",
       "      fisiologico  psiquico  comportamental  label  validado_espec  \\\n",
       "0               1         1               0      1               1   \n",
       "1               0         1               0      3               1   \n",
       "2               0         1               0      3               1   \n",
       "3               0         1               0      3               1   \n",
       "4               0         0               0      0               1   \n",
       "...           ...       ...             ...    ...             ...   \n",
       "4045            0         0               0      0               0   \n",
       "4046            0         0               0      0               0   \n",
       "4047            0         0               0      0               0   \n",
       "4048            0         0               0      0               0   \n",
       "4049            0         0               0      0               0   \n",
       "\n",
       "                        dh_tweet data_postagem hora_postagem  \\\n",
       "0     2019-10-09 02:32:41.000000    2019-10-09      02:32:41   \n",
       "1     2019-10-12 12:15:04.000000    2019-10-12      12:15:04   \n",
       "2     2019-10-11 23:56:04.000000    2019-10-11      23:56:04   \n",
       "3     2019-10-12 14:43:38.000000    2019-10-12      14:43:38   \n",
       "4     2019-10-11 12:10:22.000000    2019-10-11      12:10:22   \n",
       "...                          ...           ...           ...   \n",
       "4045  2019-11-01 00:58:30.000000    2019-11-01      00:58:30   \n",
       "4046  2019-10-31 17:37:21.000000    2019-10-31      17:37:21   \n",
       "4047  2019-10-31 16:07:31.000000    2019-10-31      16:07:31   \n",
       "4048  2019-11-11 11:42:42.000000    2019-11-11      11:42:42   \n",
       "4049  2019-11-05 15:43:28.000000    2019-11-05      15:43:28   \n",
       "\n",
       "                                                 tweets  \n",
       "0     dormi dia inteiro horas w despertafor tocava a...  \n",
       "1                             entendi importante tá bom  \n",
       "2                        chorando pq importante pra ngm  \n",
       "3     Sinto sempre nao importante certas pessoas man...  \n",
       "4                         Tô indo pra escola força ódio  \n",
       "...                                                 ...  \n",
       "4045                                               ousa  \n",
       "4046  aí amiga menos lembra KKKKKKK lucas noia carai...  \n",
       "4047         quero chegar casa, tomar banho enfiar cama  \n",
       "4048  vou ter dar menos café porque pá quantidade an...  \n",
       "4049                                    irmão lázaro???  \n",
       "\n",
       "[4049 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweets\"] = df[\"tweets\"].apply(lemmatize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWords = CountVectorizer()\n",
    "BagOfWords.fit_transform(df['tweets'])\n",
    "BagOfWords.get_feature_names_out()\n",
    "BagOfWords_result = BagOfWords.transform(df['tweets'])\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "#                     tfidf_vec.fit(input_df[‘text’])\n",
    "\n",
    "#                     tfidf_result = tfidf_vec.transform(input_df[‘text’])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4049x7276 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BagOfWords_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf_vec \u001b[38;5;241m=\u001b[39m TfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m tfidf_vec\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m tfidf_result \u001b[38;5;241m=\u001b[39m tfidf_vec\u001b[38;5;241m.\u001b[39mtransform(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m tfidf_vec_test \u001b[38;5;241m=\u001b[39m TfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vec.fit(train['tweets'])\n",
    "tfidf_result = tfidf_vec.transform(train['tweets'])\n",
    "\n",
    "tfidf_vec_test = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vec_test.fit(test['tweets'])\n",
    "tfidf_result_test = tfidf_vec.transform(train['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtfidf_result\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_result' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARA OS CONJUNTOS DE TREINAMENTO E TESTE\n",
    "X = BagOfWords_result   #Caso necessite usar o metodo TFIDF apenas trocar essa variavel por tfidf_result\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[548  17  13  76]\n",
      " [ 13  85   4   6]\n",
      " [  5   2  88  18]\n",
      " [ 59   7  19 255]]\n",
      "ACURACIDADE\n",
      "0.8032921810699588\n"
     ]
    }
   ],
   "source": [
    "### TESTE COM ARVORE DE DECISÃO\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "#Acuracidade\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSAO\n",
      "[[541  22  10  81]\n",
      " [ 17  79   0  12]\n",
      " [  6   2  89  16]\n",
      " [ 50   3  10 277]]\n",
      "ACURACIDADE\n",
      "0.811522633744856\n"
     ]
    }
   ],
   "source": [
    "### TESTE COM ENSEMBLE RANDOM FOREST\n",
    "\n",
    "rnd_mdl = RandomForestClassifier()\n",
    "rnd_mdl.fit(X_train, y_train)\n",
    "#Using the fitted model to predict from the test data\n",
    "\n",
    "#test_df is the test data and tfidf_result_test is the preprocessed test text data\n",
    "\n",
    "y_pred = rnd_mdl.predict(X_test)\n",
    "\n",
    "#Acuracidade \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 3 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[551  22  11  70]\n",
      " [ 37  60   0  11]\n",
      " [ 29   4  67  13]\n",
      " [111   5  11 213]]\n",
      "ACURACIDADE\n",
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# TESTE COM KNN\n",
    "knn = KNeighborsClassifier(3)\n",
    "\n",
    "X = BagOfWords_result\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "#Acuracidade\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[587  10   7  50]\n",
      " [ 23  78   0   7]\n",
      " [ 18   2  75  18]\n",
      " [ 81   4  10 245]]\n",
      "ACURACIDADE\n",
      "0.8106995884773662\n"
     ]
    }
   ],
   "source": [
    "## TESTE COM REGRESSÃO LOGISTICA\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "lr.fit(X_train, y_train)                  # Emprega o conjunto de treinamento \n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
