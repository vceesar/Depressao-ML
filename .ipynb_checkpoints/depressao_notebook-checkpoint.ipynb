{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import configparser as cp\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#ALGORITMOS SELECIONADOS AT√â O MOMENTO\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#-----------------------------------------\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = cp.ConfigParser()\n",
    "config.read('config.ini')\n",
    "api_key = config['twitter_credenciais']['api_key']\n",
    "api_key_secret = config['twitter_credenciais']['api_key_secret']\n",
    "access_token = config['twitter_credenciais']['access_token']\n",
    "access_token_secret = config['twitter_credenciais']['access_token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_depressao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tweet</th>\n",
       "      <th>ds_tweet</th>\n",
       "      <th>fisiologico</th>\n",
       "      <th>psiquico</th>\n",
       "      <th>comportamental</th>\n",
       "      <th>label</th>\n",
       "      <th>validado_espec</th>\n",
       "      <th>dh_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3696</td>\n",
       "      <td>dormi o dia inteiro tinha horas w meu desperta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-09 02:32:41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885</td>\n",
       "      <td>j√° entendi que eu n√£o sou importante t√° bom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 12:15:04.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3888</td>\n",
       "      <td>chorando pq n√£o sou importante pra ngm üëçüèø http...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 23:56:04.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3889</td>\n",
       "      <td>Sinto sempre que nao sou importante para certa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 14:43:38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8369</td>\n",
       "      <td>T√¥ indo pra escola com a for√ßa do √≥dio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 12:10:22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>10629</td>\n",
       "      <td>@LUCAztec como ousa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01 00:58:30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>10630</td>\n",
       "      <td>@tatastew a√≠ amiga mas pelo menos ele lembra K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 17:37:21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>10631</td>\n",
       "      <td>eu s√≥ quero chegar em casa, tomar um banho e m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 16:07:31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>10632</td>\n",
       "      <td>vou ter que dar um menos no caf√© porque se p√° ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-11 11:42:42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>10633</td>\n",
       "      <td>@lxcxzs irm√£o l√°zaro???</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 15:43:28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4050 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_tweet                                           ds_tweet  \\\n",
       "0         3696  dormi o dia inteiro tinha horas w meu desperta...   \n",
       "1         3885        j√° entendi que eu n√£o sou importante t√° bom   \n",
       "2         3888  chorando pq n√£o sou importante pra ngm üëçüèø http...   \n",
       "3         3889  Sinto sempre que nao sou importante para certa...   \n",
       "4         8369             T√¥ indo pra escola com a for√ßa do √≥dio   \n",
       "...        ...                                                ...   \n",
       "4045     10629                                @LUCAztec como ousa   \n",
       "4046     10630  @tatastew a√≠ amiga mas pelo menos ele lembra K...   \n",
       "4047     10631  eu s√≥ quero chegar em casa, tomar um banho e m...   \n",
       "4048     10632  vou ter que dar um menos no caf√© porque se p√° ...   \n",
       "4049     10633                            @lxcxzs irm√£o l√°zaro???   \n",
       "\n",
       "      fisiologico  psiquico  comportamental  label  validado_espec  \\\n",
       "0               1         1               0      1               1   \n",
       "1               0         1               0      3               1   \n",
       "2               0         1               0      3               1   \n",
       "3               0         1               0      3               1   \n",
       "4               0         0               0      0               1   \n",
       "...           ...       ...             ...    ...             ...   \n",
       "4045            0         0               0      0               0   \n",
       "4046            0         0               0      0               0   \n",
       "4047            0         0               0      0               0   \n",
       "4048            0         0               0      0               0   \n",
       "4049            0         0               0      0               0   \n",
       "\n",
       "                        dh_tweet  \n",
       "0     2019-10-09 02:32:41.000000  \n",
       "1     2019-10-12 12:15:04.000000  \n",
       "2     2019-10-11 23:56:04.000000  \n",
       "3     2019-10-12 14:43:38.000000  \n",
       "4     2019-10-11 12:10:22.000000  \n",
       "...                          ...  \n",
       "4045  2019-11-01 00:58:30.000000  \n",
       "4046  2019-10-31 17:37:21.000000  \n",
       "4047  2019-10-31 16:07:31.000000  \n",
       "4048  2019-11-11 11:42:42.000000  \n",
       "4049  2019-11-05 15:43:28.000000  \n",
       "\n",
       "[4050 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_postagem'] = pd.to_datetime(df['dh_tweet']).dt.date\n",
    "df['hora_postagem'] = pd.to_datetime(df['dh_tweet']).dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_tweets(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9]_+', '', text)\n",
    "    text = re.sub(r'@_[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'@_[A-Za-z0-9]_+', '', text)# Remove mencoes\n",
    "    text = re.sub(r'#','', text) # Remove simbolo de hashtags\n",
    "    text = re.sub(r'RT[\\s]+','',text) # Remove Retweets\n",
    "    text = re.sub(r'https?:/\\/\\S+','',text) # Remove URLS\n",
    "\n",
    "    regrex_pattern = re.compile(pattern = \"[\"   #Remove todos os emojis\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4050"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweets'] = df['ds_tweet'].apply(limpar_tweets)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4049"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(df[df['tweets'] == ''].index)\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'a', 'o', 'que', 'e', '√©', 'do', 'da', 'em', 'um', 'para', 'com', 'n√£o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo', 'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™', 'essa', 'num', 'nem', 'suas', 'meu', '√†s', 'minha', 'numa', 'pelos', 'elas', 'qual', 'n√≥s', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'est√°', 'estamos', 'est√£o', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'est√°vamos', 'estavam', 'estivera', 'estiv√©ramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estiv√©ssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'h√°', 'havemos', 'h√£o', 'houve', 'houvemos', 'houveram', 'houvera', 'houv√©ramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houv√©ssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houver√°', 'houveremos', 'houver√£o', 'houveria', 'houver√≠amos', 'houveriam', 'sou', 'somos', 's√£o', 'era', '√©ramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'f√¥ramos', 'seja', 'sejamos', 'sejam', 'fosse', 'f√¥ssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'ser√°', 'seremos', 'ser√£o', 'seria', 'ser√≠amos', 'seriam', 'tenho', 'tem', 'temos', 't√©m', 'tinha', 't√≠nhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tiv√©ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tiv√©ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'ter√°', 'teremos', 'ter√£o', 'teria', 'ter√≠amos', 'teriam']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"portuguese\")\n",
    "\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removendo os stopwords\n",
    "df[\"tweets\"] = df[\"tweets\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       dormi dia inteiro horas w despertafor tocava a...\n",
       "1                               entendi importante t√° bom\n",
       "2                          chorando pq importante pra ngm\n",
       "3       Sinto sempre nao importante certas pessoas man...\n",
       "4                           T√¥ indo pra escola for√ßa √≥dio\n",
       "                              ...                        \n",
       "4045                                                 ousa\n",
       "4046    a√≠ amiga menos lembra KKKKKKK lucas noia carai...\n",
       "4047           quero chegar casa, tomar banho enfiar cama\n",
       "4048    vou ter dar menos caf√© porque p√° quantidade an...\n",
       "4049                                      irm√£o l√°zaro???\n",
       "Name: tweets, Length: 4049, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[\"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "    return \" \".join(lemma_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_tweet</th>\n",
       "      <th>ds_tweet</th>\n",
       "      <th>fisiologico</th>\n",
       "      <th>psiquico</th>\n",
       "      <th>comportamental</th>\n",
       "      <th>label</th>\n",
       "      <th>validado_espec</th>\n",
       "      <th>dh_tweet</th>\n",
       "      <th>data_postagem</th>\n",
       "      <th>hora_postagem</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3696</td>\n",
       "      <td>dormi o dia inteiro tinha horas w meu desperta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-09 02:32:41.000000</td>\n",
       "      <td>2019-10-09</td>\n",
       "      <td>02:32:41</td>\n",
       "      <td>dormi dia inteiro horas w despertafor tocava a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3885</td>\n",
       "      <td>j√° entendi que eu n√£o sou importante t√° bom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 12:15:04.000000</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>12:15:04</td>\n",
       "      <td>entendi importante t√° bom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3888</td>\n",
       "      <td>chorando pq n√£o sou importante pra ngm üëçüèø http...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 23:56:04.000000</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>23:56:04</td>\n",
       "      <td>chorando pq importante pra ngm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3889</td>\n",
       "      <td>Sinto sempre que nao sou importante para certa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-12 14:43:38.000000</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>14:43:38</td>\n",
       "      <td>Sinto sempre nao importante certas pessoas man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8369</td>\n",
       "      <td>T√¥ indo pra escola com a for√ßa do √≥dio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-11 12:10:22.000000</td>\n",
       "      <td>2019-10-11</td>\n",
       "      <td>12:10:22</td>\n",
       "      <td>T√¥ indo pra escola for√ßa √≥dio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>10629</td>\n",
       "      <td>@LUCAztec como ousa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01 00:58:30.000000</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>00:58:30</td>\n",
       "      <td>ousa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>10630</td>\n",
       "      <td>@tatastew a√≠ amiga mas pelo menos ele lembra K...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 17:37:21.000000</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>17:37:21</td>\n",
       "      <td>a√≠ amiga menos lembra KKKKKKK lucas noia carai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>10631</td>\n",
       "      <td>eu s√≥ quero chegar em casa, tomar um banho e m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-31 16:07:31.000000</td>\n",
       "      <td>2019-10-31</td>\n",
       "      <td>16:07:31</td>\n",
       "      <td>quero chegar casa, tomar banho enfiar cama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>10632</td>\n",
       "      <td>vou ter que dar um menos no caf√© porque se p√° ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-11 11:42:42.000000</td>\n",
       "      <td>2019-11-11</td>\n",
       "      <td>11:42:42</td>\n",
       "      <td>vou ter dar menos caf√© porque p√° quantidade an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>10633</td>\n",
       "      <td>@lxcxzs irm√£o l√°zaro???</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-05 15:43:28.000000</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>15:43:28</td>\n",
       "      <td>irm√£o l√°zaro???</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4049 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_tweet                                           ds_tweet  \\\n",
       "0         3696  dormi o dia inteiro tinha horas w meu desperta...   \n",
       "1         3885        j√° entendi que eu n√£o sou importante t√° bom   \n",
       "2         3888  chorando pq n√£o sou importante pra ngm üëçüèø http...   \n",
       "3         3889  Sinto sempre que nao sou importante para certa...   \n",
       "4         8369             T√¥ indo pra escola com a for√ßa do √≥dio   \n",
       "...        ...                                                ...   \n",
       "4045     10629                                @LUCAztec como ousa   \n",
       "4046     10630  @tatastew a√≠ amiga mas pelo menos ele lembra K...   \n",
       "4047     10631  eu s√≥ quero chegar em casa, tomar um banho e m...   \n",
       "4048     10632  vou ter que dar um menos no caf√© porque se p√° ...   \n",
       "4049     10633                            @lxcxzs irm√£o l√°zaro???   \n",
       "\n",
       "      fisiologico  psiquico  comportamental  label  validado_espec  \\\n",
       "0               1         1               0      1               1   \n",
       "1               0         1               0      3               1   \n",
       "2               0         1               0      3               1   \n",
       "3               0         1               0      3               1   \n",
       "4               0         0               0      0               1   \n",
       "...           ...       ...             ...    ...             ...   \n",
       "4045            0         0               0      0               0   \n",
       "4046            0         0               0      0               0   \n",
       "4047            0         0               0      0               0   \n",
       "4048            0         0               0      0               0   \n",
       "4049            0         0               0      0               0   \n",
       "\n",
       "                        dh_tweet data_postagem hora_postagem  \\\n",
       "0     2019-10-09 02:32:41.000000    2019-10-09      02:32:41   \n",
       "1     2019-10-12 12:15:04.000000    2019-10-12      12:15:04   \n",
       "2     2019-10-11 23:56:04.000000    2019-10-11      23:56:04   \n",
       "3     2019-10-12 14:43:38.000000    2019-10-12      14:43:38   \n",
       "4     2019-10-11 12:10:22.000000    2019-10-11      12:10:22   \n",
       "...                          ...           ...           ...   \n",
       "4045  2019-11-01 00:58:30.000000    2019-11-01      00:58:30   \n",
       "4046  2019-10-31 17:37:21.000000    2019-10-31      17:37:21   \n",
       "4047  2019-10-31 16:07:31.000000    2019-10-31      16:07:31   \n",
       "4048  2019-11-11 11:42:42.000000    2019-11-11      11:42:42   \n",
       "4049  2019-11-05 15:43:28.000000    2019-11-05      15:43:28   \n",
       "\n",
       "                                                 tweets  \n",
       "0     dormi dia inteiro horas w despertafor tocava a...  \n",
       "1                             entendi importante t√° bom  \n",
       "2                        chorando pq importante pra ngm  \n",
       "3     Sinto sempre nao importante certas pessoas man...  \n",
       "4                         T√¥ indo pra escola for√ßa √≥dio  \n",
       "...                                                 ...  \n",
       "4045                                               ousa  \n",
       "4046  a√≠ amiga menos lembra KKKKKKK lucas noia carai...  \n",
       "4047         quero chegar casa, tomar banho enfiar cama  \n",
       "4048  vou ter dar menos caf√© porque p√° quantidade an...  \n",
       "4049                                    irm√£o l√°zaro???  \n",
       "\n",
       "[4049 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweets\"] = df[\"tweets\"].apply(lemmatize_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagOfWords = CountVectorizer()\n",
    "BagOfWords.fit_transform(df['tweets'])\n",
    "BagOfWords.get_feature_names_out()\n",
    "BagOfWords_result = BagOfWords.transform(df['tweets'])\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "\n",
    "#                     tfidf_vec.fit(input_df[‚Äòtext‚Äô])\n",
    "\n",
    "#                     tfidf_result = tfidf_vec.transform(input_df[‚Äòtext‚Äô])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4049x7276 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 32295 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BagOfWords_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf_vec \u001b[38;5;241m=\u001b[39m TfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m tfidf_vec\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m tfidf_result \u001b[38;5;241m=\u001b[39m tfidf_vec\u001b[38;5;241m.\u001b[39mtransform(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m tfidf_vec_test \u001b[38;5;241m=\u001b[39m TfidfVectorizer(use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vec.fit(train['tweets'])\n",
    "tfidf_result = tfidf_vec.transform(train['tweets'])\n",
    "\n",
    "tfidf_vec_test = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vec_test.fit(test['tweets'])\n",
    "tfidf_result_test = tfidf_vec.transform(train['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtfidf_result\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_result' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEPARA OS CONJUNTOS DE TREINAMENTO E TESTE\n",
    "X = BagOfWords_result   #Caso necessite usar o metodo TFIDF apenas trocar essa variavel por tfidf_result\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[548  17  13  76]\n",
      " [ 13  85   4   6]\n",
      " [  5   2  88  18]\n",
      " [ 59   7  19 255]]\n",
      "ACURACIDADE\n",
      "0.8032921810699588\n"
     ]
    }
   ],
   "source": [
    "### TESTE COM ARVORE DE DECIS√ÉO\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "#Acuracidade\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIZ DE CONFUSAO\n",
      "[[541  22  10  81]\n",
      " [ 17  79   0  12]\n",
      " [  6   2  89  16]\n",
      " [ 50   3  10 277]]\n",
      "ACURACIDADE\n",
      "0.811522633744856\n"
     ]
    }
   ],
   "source": [
    "### TESTE COM ENSEMBLE RANDOM FOREST\n",
    "\n",
    "rnd_mdl = RandomForestClassifier()\n",
    "rnd_mdl.fit(X_train, y_train)\n",
    "#Using the fitted model to predict from the test data\n",
    "\n",
    "#test_df is the test data and tfidf_result_test is the preprocessed test text data\n",
    "\n",
    "y_pred = rnd_mdl.predict(X_test)\n",
    "\n",
    "#Acuracidade \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 3 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[551  22  11  70]\n",
      " [ 37  60   0  11]\n",
      " [ 29   4  67  13]\n",
      " [111   5  11 213]]\n",
      "ACURACIDADE\n",
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "# TESTE COM KNN\n",
    "knn = KNeighborsClassifier(3)\n",
    "\n",
    "X = BagOfWords_result\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "#Acuracidade\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 2 0]\n",
      "MATRIZ DE CONFUSAO\n",
      "[[587  10   7  50]\n",
      " [ 23  78   0   7]\n",
      " [ 18   2  75  18]\n",
      " [ 81   4  10 245]]\n",
      "ACURACIDADE\n",
      "0.8106995884773662\n"
     ]
    }
   ],
   "source": [
    "## TESTE COM REGRESS√ÉO LOGISTICA\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "lr.fit(X_train, y_train)                  # Emprega o conjunto de treinamento \n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"MATRIZ DE CONFUSAO\")\n",
    "print(cm)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ACURACIDADE\")\n",
    "print(accuracy)\n",
    "\n",
    "#F1-SCORE\n",
    "\n",
    "#f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
